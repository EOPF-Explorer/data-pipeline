apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: geozarr-pipeline
  namespace: devseed-staging
spec:
    # Service account with S3 and STAC API permissions
  serviceAccountName: operate-workflow-sa
  entrypoint: main
  # Clean up completed workflows after 24 hours
  ttlStrategy:
    secondsAfterCompletion: 86400  # 24 hours
  # Keep pods on failure for debugging
  podGC:
    strategy: OnWorkflowSuccess
  arguments:
    parameters:
    - name: source_url
    - name: item_id
    - name: register_collection
      value: "sentinel-2-l2a-dp-test"
    - name: stac_api_url
      value: "https://api.explorer.eopf.copernicus.eu/stac"
    - name: raster_api_url
      value: "https://api.explorer.eopf.copernicus.eu/raster"
    - name: s3_endpoint
      value: "https://s3.de.io.cloud.ovh.net"
    - name: s3_output_bucket
      value: "esa-zarr-sentinel-explorer-fra"
    - name: s3_output_prefix
      value: "tests-output"
    - name: pipeline_image_version
      value: "v26"  # v26 includes Dask parallel processing

  templates:
  - name: main
    dag:
      tasks:
      - name: convert
        template: convert-geozarr
      - name: validate
        template: validate
        dependencies: [convert]
      - name: register
        template: register-stac
        dependencies: [validate]
      - name: augment
        template: augment-stac
        dependencies: [register]

  - name: convert-geozarr
    activeDeadlineSeconds: 3600  # 1 hour timeout
    script:
      image: ghcr.io/eopf-explorer/data-pipeline:{{workflow.parameters.pipeline_image_version}}
      imagePullPolicy: Always
      command: [bash]
      source: |
        set -euo pipefail

        SOURCE_URL="{{workflow.parameters.source_url}}"
        COLLECTION="{{workflow.parameters.register_collection}}"
        OUTPUT_PATH="s3://{{workflow.parameters.s3_output_bucket}}/{{workflow.parameters.s3_output_prefix}}/$COLLECTION/{{workflow.parameters.item_id}}.zarr"

        echo "üîç Resolving source..."
        # Check if source is STAC item or direct zarr
        if [[ "$SOURCE_URL" == *"/items/"* ]]; then
          echo "üì° Extracting Zarr URL from STAC item..."
          ZARR_URL=$(python3 /app/scripts/get_zarr_url.py "$SOURCE_URL")
          echo "‚úÖ Zarr URL: $ZARR_URL"
        else
          ZARR_URL="$SOURCE_URL"
          echo "‚úÖ Direct Zarr URL: $ZARR_URL"
        fi

        echo "üöÄ Starting GeoZarr conversion"
        echo "Source: $ZARR_URL"
        echo "Destination: $OUTPUT_PATH"
        echo "Collection: $COLLECTION"

        # Clean up any partial output from previous failed runs (optional)
        if [ -f /app/scripts/cleanup_s3_path.py ]; then
          echo "üßπ Cleaning up any existing output..."
          python3 /app/scripts/cleanup_s3_path.py "$OUTPUT_PATH" || echo "‚ö†Ô∏è  Cleanup failed, continuing anyway"
        else
          echo "‚ÑπÔ∏è  Skipping cleanup (script not available)"
        fi

        # Get collection-specific conversion parameters from registry
        echo "üìã Getting conversion parameters for $COLLECTION..."
        eval $(python3 /app/scripts/get_conversion_params.py --collection "$COLLECTION")

        echo "üì° Conversion mode:"
        echo "  Groups: $ZARR_GROUPS"
        echo "  Chunk: $CHUNK"
        echo "  Tile width: $TILE_WIDTH"
        echo "  Extra flags: $EXTRA_FLAGS"

        # Build conversion command with Dask for parallel processing
        eopf-geozarr convert "$ZARR_URL" "$OUTPUT_PATH" \
          --groups "$ZARR_GROUPS" \
          $EXTRA_FLAGS \
          --spatial-chunk $CHUNK \
          --tile-width $TILE_WIDTH \
          --dask-cluster \
          --verbose
      env:
      - name: PYTHONUNBUFFERED
        value: "1"
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            name: geozarr-s3-credentials
            key: AWS_ACCESS_KEY_ID
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: geozarr-s3-credentials
            key: AWS_SECRET_ACCESS_KEY
      - name: AWS_ENDPOINT_URL
        value: "{{workflow.parameters.s3_endpoint}}"
      resources:
        requests:
          memory: "8Gi"
          cpu: "1"
        limits:
          memory: "16Gi"
          cpu: "4"

  - name: validate
    activeDeadlineSeconds: 300  # 5 min timeout
    container:
      image: ghcr.io/eopf-explorer/data-pipeline:{{workflow.parameters.pipeline_image_version}}
      imagePullPolicy: Always
      command: [python]
      args:
      - /app/scripts/validate_geozarr.py
      - "s3://{{workflow.parameters.s3_output_bucket}}/{{workflow.parameters.s3_output_prefix}}/{{workflow.parameters.register_collection}}/{{workflow.parameters.item_id}}.zarr"
      - --item-id
      - "{{workflow.parameters.item_id}}"
      - --verbose
      env:
      - name: PYTHONUNBUFFERED
        value: "1"
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            name: geozarr-s3-credentials
            key: AWS_ACCESS_KEY_ID
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: geozarr-s3-credentials
            key: AWS_SECRET_ACCESS_KEY
      - name: AWS_ENDPOINT_URL
        value: "{{workflow.parameters.s3_endpoint}}"
      - name: ZARR_V3_EXPERIMENTAL_API
        value: "1"
      resources:
        requests:
          memory: "2Gi"
        limits:
          memory: "4Gi"

  - name: register-stac
    activeDeadlineSeconds: 300  # 5 min timeout
    container:
      # Use data-pipeline image for Python scripts (register, augment)
      image: ghcr.io/eopf-explorer/data-pipeline:{{workflow.parameters.pipeline_image_version}}
      imagePullPolicy: Always
      command: [python]
      args:
      - /app/scripts/register_stac.py
      - --stac
      - "{{workflow.parameters.stac_api_url}}"
      - --collection
      - "{{workflow.parameters.register_collection}}"
      - --item-id
      - "{{workflow.parameters.item_id}}"
      - --output
      - "s3://{{workflow.parameters.s3_output_bucket}}/{{workflow.parameters.s3_output_prefix}}/{{workflow.parameters.register_collection}}/{{workflow.parameters.item_id}}.zarr"
      - --src-item
      - "{{workflow.parameters.source_url}}"
      - --s3-endpoint
      - "{{workflow.parameters.s3_endpoint}}"
      - --mode
      - "update"
      env:
      - name: PYTHONUNBUFFERED
        value: "1"

  - name: augment-stac
    activeDeadlineSeconds: 300  # 5 min timeout
    container:
      # Use data-pipeline image for Python scripts (register, augment)
      image: ghcr.io/eopf-explorer/data-pipeline:{{workflow.parameters.pipeline_image_version}}
      imagePullPolicy: Always
      command: [python]
      args:
      - /app/scripts/augment_stac_item.py
      - --stac
      - "{{workflow.parameters.stac_api_url}}"
      - --raster-base
      - "{{workflow.parameters.raster_api_url}}"
      - --collection
      - "{{workflow.parameters.register_collection}}"
      - --item-id
      - "{{workflow.parameters.item_id}}"
      - --verbose
      env:
      - name: PYTHONUNBUFFERED
        value: "1"

  # Workflow-level metadata to ensure UI visibility
  workflowMetadata:
    labels:
      workflows.argoproj.io/workflow-template: geozarr-pipeline
