apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: geozarr-pipeline
  namespace: devseed-staging
spec:
  # Service account with S3 and STAC API permissions
  serviceAccountName: operate-workflow-sa
  entrypoint: main
  # Disable log archival - logs visible directly in UI without S3 archival delay
  archiveLogs: false
  # Clean up completed workflows after 24 hours
  ttlStrategy:
    secondsAfterCompletion: 86400  # 24 hours
  # Keep pods on failure for debugging
  podGC:
    strategy: OnWorkflowSuccess
  # Add workflow metadata labels for easier filtering in UI
  workflowMetadata:
    labels:
      workflows.argoproj.io/workflow-template: geozarr-pipeline
      pipeline.eopf/collection: "{{workflow.parameters.register_collection}}"
      pipeline.eopf/item-id: "{{workflow.parameters.item_id}}"
  arguments:
    parameters:
    - name: source_url
    - name: item_id
    - name: register_collection
      value: "sentinel-2-l2a-dp-test"
    - name: stac_api_url
      value: "https://api.explorer.eopf.copernicus.eu/stac"
    - name: raster_api_url
      value: "https://api.explorer.eopf.copernicus.eu/raster"
    - name: s3_endpoint
      value: "https://s3.de.io.cloud.ovh.net"
    - name: s3_output_bucket
      value: "esa-zarr-sentinel-explorer-fra"
    - name: s3_output_prefix
      value: "tests-output"
    - name: pipeline_image_version
      value: "v26"  # v26 includes Dask parallel processing

  templates:
  - name: main
    dag:
      tasks:
      - name: show-parameters
        template: show-parameters
      - name: convert
        template: convert-geozarr
        dependencies: [show-parameters]
      - name: validate
        template: validate
        dependencies: [convert]
      - name: register
        template: register-stac
        dependencies: [validate]
      - name: augment
        template: augment-stac
        dependencies: [register]

  - name: show-parameters
    activeDeadlineSeconds: 60
    container:
      image: ghcr.io/eopf-explorer/data-pipeline:{{workflow.parameters.pipeline_image_version}}
      imagePullPolicy: Always
      command: ["/bin/sh"]
      args:
        - -c
        - |
          echo "=== Workflow Parameters ==="
          echo "{{workflow.parameters}}"

  - name: convert-geozarr
    activeDeadlineSeconds: 3600  # 1 hour timeout
    container:
      image: ghcr.io/eopf-explorer/data-pipeline:{{workflow.parameters.pipeline_image_version}}
      imagePullPolicy: Always
      command: [bash, -c]
      resources:
        requests:
          memory: "6Gi"
          cpu: "2"
        limits:
          memory: "10Gi"
          cpu: "4"
      args:
        - |
          set -euo pipefail

          echo "════════════════════════════════════════════════════════════════════════════"
          echo "  STEP 1/4: GEOZARR CONVERSION"
          echo "════════════════════════════════════════════════════════════════════════════"
          echo ""

          SOURCE_URL="{{workflow.parameters.source_url}}"
          COLLECTION="{{workflow.parameters.register_collection}}"
          OUTPUT_PATH="s3://{{workflow.parameters.s3_output_bucket}}/{{workflow.parameters.s3_output_prefix}}/$COLLECTION/{{workflow.parameters.item_id}}.zarr"

          echo "🔍 [1/6] Resolving source..."
          # Check if source is STAC item or direct zarr
          if [[ "$SOURCE_URL" == *"/items/"* ]]; then
            echo "📡 Extracting Zarr URL from STAC item..."
            ZARR_URL=$(python3 /app/scripts/get_zarr_url.py "$SOURCE_URL")
            echo "✅ Zarr URL: $ZARR_URL"
          else
            ZARR_URL="$SOURCE_URL"
            echo "✅ Direct Zarr URL: $ZARR_URL"
          fi
          echo ""

          echo "� [2/6] Getting conversion parameters for $COLLECTION..."
          eval $(python3 /app/scripts/get_conversion_params.py --collection "$COLLECTION")
          echo "   Groups:      $ZARR_GROUPS"
          echo "   Chunk:       $CHUNK"
          echo "   Tile width:  $TILE_WIDTH"
          echo "   Extra flags: $EXTRA_FLAGS"
          echo ""

          echo "🧹 [3/6] Cleaning up existing output..."
          if [ -f /app/scripts/cleanup_s3_path.py ]; then
            python3 /app/scripts/cleanup_s3_path.py "$OUTPUT_PATH" || echo "⚠️  Cleanup failed (may not exist yet)"
          else
            echo "ℹ️  Skipping cleanup (script not available)"
          fi
          echo ""

          echo "🚀 [4/6] Starting GeoZarr conversion..."
          echo "   Source:      $ZARR_URL"
          echo "   Destination: $OUTPUT_PATH"
          echo "   Collection:  $COLLECTION"
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "  CONVERSION LOGS (parallel processing with local Dask cluster)"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo ""

          # Build conversion command with parallel processing
          # - Enable local Dask cluster for parallel chunk processing
          # - Higher CPU/memory resources support multiple Dask workers
          eopf-geozarr convert "$ZARR_URL" "$OUTPUT_PATH" \
            --groups "$ZARR_GROUPS" \
            $EXTRA_FLAGS \
            --spatial-chunk $CHUNK \
            --tile-width $TILE_WIDTH \
            --dask-cluster \
            --verbose

          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "✅ [6/6] Conversion completed successfully!"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
      env:
      - name: PYTHONUNBUFFERED
        value: "1"
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            name: geozarr-s3-credentials
            key: AWS_ACCESS_KEY_ID
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: geozarr-s3-credentials
            key: AWS_SECRET_ACCESS_KEY
      - name: AWS_ENDPOINT_URL
        value: "{{workflow.parameters.s3_endpoint}}"
      resources:
        requests:
          memory: "6Gi"
          cpu: "2"
        limits:
          memory: "10Gi"
          cpu: "4"

  - name: validate
    activeDeadlineSeconds: 300  # 5 min timeout
    script:
      image: ghcr.io/eopf-explorer/data-pipeline:{{workflow.parameters.pipeline_image_version}}
      imagePullPolicy: Always
      command: [bash]
      resources:
        requests:
          memory: "2Gi"
          cpu: "1"
        limits:
          memory: "4Gi"
          cpu: "2"
      source: |
        set -euo pipefail

        echo "════════════════════════════════════════════════════════════════════════════"
        echo "  STEP 2/4: GEOZARR VALIDATION"
        echo "════════════════════════════════════════════════════════════════════════════"
        echo ""
        echo "🔍 Validating GeoZarr structure and compliance..."
        echo ""

        python /app/scripts/validate_geozarr.py \
          "s3://{{workflow.parameters.s3_output_bucket}}/{{workflow.parameters.s3_output_prefix}}/{{workflow.parameters.register_collection}}/{{workflow.parameters.item_id}}.zarr" \
          --item-id "{{workflow.parameters.item_id}}" \
          --verbose

        echo ""
        echo "✅ Validation completed successfully!"
      env:
      - name: PYTHONUNBUFFERED
        value: "1"
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            name: geozarr-s3-credentials
            key: AWS_ACCESS_KEY_ID
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            name: geozarr-s3-credentials
            key: AWS_SECRET_ACCESS_KEY
      - name: AWS_ENDPOINT_URL
        value: "{{workflow.parameters.s3_endpoint}}"
      - name: ZARR_V3_EXPERIMENTAL_API
        value: "1"
      resources:
        requests:
          memory: "2Gi"
        limits:
          memory: "4Gi"

  - name: register-stac
    activeDeadlineSeconds: 300  # 5 min timeout
    script:
      image: ghcr.io/eopf-explorer/data-pipeline:{{workflow.parameters.pipeline_image_version}}
      imagePullPolicy: Always
      command: [bash]
      resources:
        requests:
          memory: "1Gi"
          cpu: "500m"
        limits:
          memory: "2Gi"
          cpu: "1"
      source: |
        set -euo pipefail

        echo "════════════════════════════════════════════════════════════════════════════"
        echo "  STEP 3/4: STAC REGISTRATION"
        echo "════════════════════════════════════════════════════════════════════════════"
        echo ""
        echo "📝 Registering item in STAC API..."
        echo "   Collection: {{workflow.parameters.register_collection}}"
        echo "   Item ID:    {{workflow.parameters.item_id}}"
        echo "   STAC API:   {{workflow.parameters.stac_api_url}}"
        echo ""

        python /app/scripts/register_stac.py \
          --stac "{{workflow.parameters.stac_api_url}}" \
          --collection "{{workflow.parameters.register_collection}}" \
          --item-id "{{workflow.parameters.item_id}}" \
          --output "s3://{{workflow.parameters.s3_output_bucket}}/{{workflow.parameters.s3_output_prefix}}/{{workflow.parameters.register_collection}}/{{workflow.parameters.item_id}}.zarr" \
          --src-item "{{workflow.parameters.source_url}}" \
          --s3-endpoint "{{workflow.parameters.s3_endpoint}}" \
          --mode "update"

        echo ""
        echo "✅ Registration completed successfully!"
      env:
      - name: PYTHONUNBUFFERED
        value: "1"

  - name: augment-stac
    activeDeadlineSeconds: 300  # 5 min timeout
    script:
      image: ghcr.io/eopf-explorer/data-pipeline:{{workflow.parameters.pipeline_image_version}}
      imagePullPolicy: Always
      command: [bash]
      resources:
        requests:
          memory: "1Gi"
          cpu: "500m"
        limits:
          memory: "2Gi"
          cpu: "1"
      source: |
      source: |
        set -euo pipefail

        echo "════════════════════════════════════════════════════════════════════════════"
        echo "  STEP 4/4: STAC AUGMENTATION"
        echo "════════════════════════════════════════════════════════════════════════════"
        echo ""
        echo "🎨 Adding preview links and metadata to STAC item..."
        echo "   Collection:  {{workflow.parameters.register_collection}}"
        echo "   Item ID:     {{workflow.parameters.item_id}}"
        echo "   Raster API:  {{workflow.parameters.raster_api_url}}"
        echo ""

        python /app/scripts/augment_stac_item.py \
          --stac "{{workflow.parameters.stac_api_url}}" \
          --raster-base "{{workflow.parameters.raster_api_url}}" \
          --collection "{{workflow.parameters.register_collection}}" \
          --item-id "{{workflow.parameters.item_id}}" \
          --verbose

        echo ""
        echo "✅ Augmentation completed successfully!"
        echo ""
        echo "════════════════════════════════════════════════════════════════════════════"
        echo "  🎉 PIPELINE COMPLETED SUCCESSFULLY!"
        echo "════════════════════════════════════════════════════════════════════════════"
        echo ""
        echo "📍 View item in STAC API:"
        echo "   {{workflow.parameters.stac_api_url}}/collections/{{workflow.parameters.register_collection}}/items/{{workflow.parameters.item_id}}"
        echo ""
        echo "📦 GeoZarr output location:"
        echo "   s3://{{workflow.parameters.s3_output_bucket}}/{{workflow.parameters.s3_output_prefix}}/{{workflow.parameters.register_collection}}/{{workflow.parameters.item_id}}.zarr"
        echo ""
      env:
      - name: PYTHONUNBUFFERED
        value: "1"

  # Workflow-level metadata to ensure UI visibility
  workflowMetadata:
    labels:
      workflows.argoproj.io/workflow-template: geozarr-pipeline
